{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment _TextAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNac5QEUpvcKZOX2tgDVU57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shi-star/Text_Sentiment_Analysis_movie_review/blob/main/Sentiment__TextAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOea5NxEPRMJ",
        "outputId": "ce0246a9-3e62-43e7-ce2c-231e58a5c59a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jtf8NYHQHpg"
      },
      "source": [
        "path = '/content/drive/MyDrive/archive.zip'"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YstrDZ8XXdfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e797a3bb-9c1e-40cd-e3a0-2e70120abdb5"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rehIxEzCtosg"
      },
      "source": [
        "1. **We have imported NLTK (Natural Language ToolKit), for stopping, stemming, lemmatization**\n",
        "2. **nltk.download('punkt') # default download,It is downloaded default as available in the package**\n",
        "3. **nltk.download('stopwords') # downloading the stopwords to be used in the stopping process**\n",
        "4. **nltk.download('wordnet') # downloading wordnet(is like imagenet for text) text data for stemming and lemmatization purpose**\n",
        "5. **from nltk.corpus import stopwords # imports the stopwords we can use**\n",
        "6. **from nltk.stem.wordnet import WordNetLemmatizer # Lemmatizer**\n",
        "7. **import re # Regular Expression, RegEx**\n",
        "8. **from sklearn.feature_extraction.text import TfidfVectorizer (to calculate TF, IDF)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeS1KHPlXXtx"
      },
      "source": [
        "txt = pd.read_csv(path, nrows = 1000)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEpP0gpsuZla"
      },
      "source": [
        "**Although the data has 25000 rows, but it didn't run well so I took only 1000 rows of the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "K_RxAbGGXcKL",
        "outputId": "646d14fc-7898-4702-d4e0-28372acec3e9"
      },
      "source": [
        "txt.head()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Positive</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Positive</td>\n",
              "      <td>'The Classic War of the Worlds' by Timothy Hin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Negative</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Negative</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentiment                                             review\n",
              "0  Positive  With all this stuff going down at the moment w...\n",
              "1  Positive  'The Classic War of the Worlds' by Timothy Hin...\n",
              "2  Negative  The film starts with a manager (Nicholas Bell)...\n",
              "3  Negative  It must be assumed that those who praised this...\n",
              "4  Positive  Superbly trashy and wondrously unpretentious 8..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E49uC0Wwuk07"
      },
      "source": [
        "**Glimpse of the Data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXSD4a6XXhIE"
      },
      "source": [
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oj9ZVr6unrc"
      },
      "source": [
        "Forgot to import :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9UhSzzMZFYQ",
        "outputId": "e648c8c7-226b-4146-9b85-05552c58fb81"
      },
      "source": [
        "txt.shape"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2oHW_G3usi5"
      },
      "source": [
        "**We have 1000 rows and 2 columns, one is target column.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXT6OIZwZK7y",
        "outputId": "32d6ad31-9a55-4b7d-acd6-20adfad448ad"
      },
      "source": [
        "txt['sentiment'].value_counts()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Negative    518\n",
              "Positive    482\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW9C0eKuu03_"
      },
      "source": [
        "**We can see we have balanced data . almost an equal number of both the sentiments**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP6lE3o9ZRC2"
      },
      "source": [
        "dict1 = {'Positive':1, 'Negative':0}"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOVyETa5u9lF"
      },
      "source": [
        "**So let's change them into numerical so that it will be easy to process further**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhyp8C7QZulg"
      },
      "source": [
        "txt['sentiment'] = txt['sentiment'].map(dict1)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxwr9bu3Z1e_",
        "outputId": "96a235be-e488-4103-d68d-4f023eb54287"
      },
      "source": [
        "txt['sentiment'].value_counts()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    518\n",
              "1    482\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "PsubGY7n4LoH",
        "outputId": "7c430fda-5059-4f6d-926e-b79a34fa57f9"
      },
      "source": [
        "txt['review'][1]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"'The Classic War of the Worlds' by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur 'critics' look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the 'critics'. We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the 'critics' perceive to be its shortcomings.'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41hEaKdVvGYD"
      },
      "source": [
        "**We can see second row of the data, and we can see each row has a lot of text in it.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zELIkfV0IX4"
      },
      "source": [
        "#**1.Creating Columns for TF-IDF table**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTONcdnc0gCw"
      },
      "source": [
        "text = ''\n",
        "\n",
        "for i in txt['review']:\n",
        "  text += re.sub('[^a-z ]', '', i.strip().lower())+' '\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJpWN2-EvNYx"
      },
      "source": [
        "**Here we have taken each row and then applied the regex function on each row to remove all the character except (a-z and a space).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCfIVsP9vzRA"
      },
      "source": [
        "# **Tokenisation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u6O_Omw0gH7"
      },
      "source": [
        "nltk.word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKsJsDqavdGH"
      },
      "source": [
        "**We have tokenised the data so that each word is now in an element of a list. it returns individual words from the given text in the form of a list, this is tokenizer\n",
        "We are using word_tokenize function of nltk here, but alternatively we can also use .split()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF8zkGBC0gJ1",
        "outputId": "122d6eb4-8c9d-4303-c785-195d459658b1"
      },
      "source": [
        "word = list(set(nltk.word_tokenize(text)))\n",
        "print(len(word))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBu_He2nvk9s"
      },
      "source": [
        "**We have taken the set here so that all the duplicates will be removed. And we can see we have 20408 words i.e columns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXKZ_Y3iwGNG"
      },
      "source": [
        "# **Removing Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi1stOpZ0gNt"
      },
      "source": [
        "filtered = list(set([w for w in word if not w in stopwords.words('english')]))\n",
        "print(len(filtered))\n",
        "print(filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbeDy8_8vqU6"
      },
      "source": [
        "**Here we have filtered the data so that all the irrelevant words will be removed. And now we can see we have 20275 columns left**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3TznOdHwRd-"
      },
      "source": [
        "# **Lammetization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2BqQuGH0gPn"
      },
      "source": [
        "wordnet_lammetizer = WordNetLemmatizer()"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQNLSrhW0gRt",
        "outputId": "71d052cb-41ae-4c49-f445-bde4672577e6"
      },
      "source": [
        "wordnet_lammetizer_v = []\n",
        "for i in filtered:\n",
        "  wordnet_lammetizer_v.append(wordnet_lammetizer.lemmatize(i,'v'))\n",
        "\n",
        "wordnet_v = list(set(wordnet_lammetizer_v))\n",
        "print(len(wordnet_v))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIRcG9sTwwAc"
      },
      "source": [
        "**After lammetization we are left with 17208 columns or unique words.As we have taken set of the lammetized word, lemmatize to remove repetitive words like play, played to reduce this number.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FOxB4eSxvn0"
      },
      "source": [
        "Till here we have created columns of unique words , using Lammetisation, tokenisation. Now we will create rows but will not take set as we do not want to remove duplicates from the rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFKOhAK0yByu"
      },
      "source": [
        "# **2.Creating Rows to calculate TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcMltM960gTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d70351-fd28-4d99-9c6e-6a34b65920ac"
      },
      "source": [
        "train_file_wordnet_v = []\n",
        "\n",
        "\n",
        "for line in txt['review']:\n",
        "    \n",
        "    x = re.sub('[^a-z ]', '', line.strip().lower())\n",
        "    tokens = nltk.word_tokenize(x)\n",
        "    filtered_tokens = [w for w in tokens if not w in stopwords.words('english')]\n",
        "    stemmed_tokens = ''\n",
        "    for item in filtered_tokens:\n",
        "        stemmed_tokens += wordnet_lammetizer.lemmatize(item, 'v')+' '\n",
        "    train_file_wordnet_v.append(stemmed_tokens[:-1]) # v means verb, we will consider play --> play and played --> play as same \n",
        "# wordnet_v is the new list which has unique non-stopwords and lemmatized words\n",
        "\n",
        "\n",
        "print(len(train_file_wordnet_v))\n",
        "print(train_file_wordnet_v[1])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "classic war worlds timothy hines entertain film obviously go great effort lengths faithfully recreate h g well classic book mr hines succeed watch film appreciate fact standard predictable hollywood fare come every year eg spielberg version tom cruise slightest resemblance book obviously everyone look different things movie envision amateur critics look criticize everything others rate movie important baseslike entertain people never agree critics enjoy effort mr hines put faithful hg well classic novel find entertain make easy overlook critics perceive shortcomings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMY45AL2yMpe"
      },
      "source": [
        "**We have cleaned the rows, like removed the stopwords, applied the tokenisation and lammetization.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PoCn80ngRHk"
      },
      "source": [
        "x = train_file_wordnet_v"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5zqQK15yg9J"
      },
      "source": [
        "**Now let's do the train_test split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY-6PkSOgocv",
        "outputId": "c4c59f62-429a-4732-b799-05647c176abb"
      },
      "source": [
        "len(x)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY2CI9-Xg4Sm"
      },
      "source": [
        "y = txt['sentiment']"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqNeoaX8ftuo"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDtQoBiNgvgx"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=0)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgkikq29ypaz"
      },
      "source": [
        "**Here we have created the object of the TF_iDF to calculate TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frtk6mJB0gWn"
      },
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI4uzHzs0gYN"
      },
      "source": [
        "X_train_tfidf = vectorizer.fit_transform(x_train)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khKCwhwDy05_"
      },
      "source": [
        "**We have fit_transform the x_train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OZCvaAI0gcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653aa9f1-ca79-4b5c-c7bf-2b556e22efa1"
      },
      "source": [
        "print(X_train_tfidf.shape)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(700, 13766)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls1fVnQk0gfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b56e672-3cf7-4364-ba3c-dd7a4052cabc"
      },
      "source": [
        "print(type(X_train_tfidf))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDgWydDJ0giO"
      },
      "source": [
        "X_train_tfidf1 = X_train_tfidf.toarray()"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDvBMxrFy-cU"
      },
      "source": [
        "**As model can not take anyother format than pd dataframe or array. so changed it to the array**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Qh-_Wpep2v"
      },
      "source": [
        "X_test_tfidf = vectorizer.transform(x_test)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCzbIVVzMZf"
      },
      "source": [
        "**Transform the X_test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDkNlEhQep1K"
      },
      "source": [
        "X_test_tfidf1 = X_test_tfidf.toarray()"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQMpaHnxzRrw"
      },
      "source": [
        "**Imported the model , we are using Random Forest , as it is a binary classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e3z2AKNepza"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ubx009Jeptr"
      },
      "source": [
        "model_parameters = {\n",
        "        'n_estimators':[ 100],\n",
        "        'criterion':['gini'],\n",
        "        'max_depth': [3, 5]\n",
        "    }"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XtSvKMMepq6",
        "outputId": "c41772c8-b4af-4206-91b3-91a500aef72b"
      },
      "source": [
        "model = RandomForestClassifier(random_state=1)\n",
        "gscv = GridSearchCV(estimator=model, \n",
        "                    param_grid=model_parameters, \n",
        "                    cv=5, \n",
        "                    verbose=1, \n",
        "                    n_jobs=-1,\n",
        "                    scoring='roc_auc')\n",
        "\n",
        "gscv.fit(X_train_tfidf1, y_train)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                              class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              max_samples=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators=100, n_jobs=None,\n",
              "                                              oob_score=False, random_state=1,\n",
              "                                              verbose=0, warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'criterion': ['gini'], 'max_depth': [3, 5],\n",
              "                         'n_estimators': [100]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='roc_auc', verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xw3k295epne",
        "outputId": "efdd1a84-8809-4e57-98ab-144360591078"
      },
      "source": [
        "print('The best parameter are -', gscv.best_params_)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameter are - {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMwHKVG9eplh",
        "outputId": "6d1cde9e-76fa-4e75-ea15-1915b2cd01a5"
      },
      "source": [
        "print(gscv.best_score_)\n",
        "print(gscv.best_estimator_)\n",
        "print(gscv.scorer_)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8394719043870547\n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=5, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
            "                       warm_start=False)\n",
            "make_scorer(roc_auc_score, needs_threshold=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViINDw6jh_aD",
        "outputId": "5fe55d51-014e-400d-f7b0-2a1359a5dd87"
      },
      "source": [
        "print('AUC on test by gscv =', roc_auc_score(y_true=y_test,\n",
        "                                                        y_score=gscv.predict_proba(X_test_tfidf1)[:, 1]))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC on test by gscv = 0.8796390950708921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox_mpe2czc9N"
      },
      "source": [
        "We can see the Roc_auc score is 87% using Random Forest with the help of the Grid Search CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Q5mN3Pwy0m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}